{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23071dc-1e7c-450b-9fcd-cec98deac88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9aa502-97bd-4015-a947-53ceb97dc2ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_test_frame():\n",
    "    video_path = '../input/video/video1.mov'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    try:\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video.\")\n",
    "            exit()\n",
    "        \n",
    "        success, first_frame = cap.read()\n",
    "        first_frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        if not success:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            exit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return first_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b28ba90-5112-446c-807f-1eb40ec728ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ball(frame, param2=40):\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.medianBlur(gray, 5)\n",
    "    rows = gray.shape[0]\n",
    "    #print(f\"Using param2 value: {param2}\")\n",
    "    circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, rows / 8,\n",
    "                               param1=100, param2=param2,\n",
    "                               minRadius=1, maxRadius=30)\n",
    "     \n",
    "    \n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "\n",
    "        for i in circles[0, :]:\n",
    "            center = (i[0], i[1])\n",
    "            # circle center\n",
    "            cv2.circle(frame, center, 1, (0, 100, 100), 3)\n",
    "            # circle outline\n",
    "            radius = i[2]\n",
    "            cv2.circle(frame, center, radius, (255, 0, 255), 3)\n",
    "    \n",
    " \n",
    "        if len(circles) > 1:\n",
    "            print(\"Error: multiple circles detected\")\n",
    "            exit()\n",
    "    #cv2.imshow(\"detected circles\", frame)\n",
    "    #cv2.waitKey(0)\n",
    "    x, y, r = circles[0, 0]  # Get center x, y and radius of first circle\n",
    "    \n",
    "    scaled_r = r * 1.2 #bounding box is best with slightly larger than exact circle diameter\n",
    "    \n",
    "    # Convert to bounding box format (x, y, width, height) where x,y is top-left corner (bounding box needs corner not center)\n",
    "    bbox = (int(x-scaled_r), int(y-scaled_r), int(2.1*scaled_r), int(2.1*scaled_r))\n",
    "    \n",
    " \n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57da69b-6be5-4187-a2e6-fa6c121840dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_track_check(position_history, current_position, stopped_frames_count=0):\n",
    "    import math\n",
    "    \n",
    "    # Configuration parameters\n",
    "    motion_threshold = 1\n",
    "    stopped_frames_threshold = 10\n",
    "    long_term_frames = 30\n",
    "    long_term_threshold = 5  # Minimum movement required over multiple frames\n",
    "    \n",
    "    # Add detailed movement logging\n",
    "    if position_history:\n",
    "        prev_position = position_history[-1]\n",
    "        dx = current_position[0] - prev_position[0]\n",
    "        dy = current_position[1] - prev_position[1]\n",
    "        distance = math.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        # Log actual values for debugging\n",
    "        #print(f\"Frame movement: dx={dx}, dy={dy}, distance={distance:.2f}\")\n",
    "        \n",
    "        # Check long-term movement if we have enough history\n",
    "        if len(position_history) >= long_term_frames:\n",
    "            old_position = position_history[-long_term_frames]\n",
    "            long_dx = current_position[0] - old_position[0]\n",
    "            long_dy = current_position[1] - old_position[1]\n",
    "            long_distance = math.sqrt(long_dx**2 + long_dy**2)\n",
    "            #print(f\"{long_term_frames}-frame movement: {long_distance:.2f} pixels\")\n",
    "            \n",
    "            # If significant movement over multiple frames, reset counter\n",
    "            if long_distance > long_term_threshold:\n",
    "                stopped_frames_count = 0\n",
    "                #print(f\"Significant movement over {long_term_frames} frames: {long_distance:.2f} pixels\")\n",
    "                \n",
    "            # Otherwise, check frame-to-frame movement\n",
    "            elif distance < motion_threshold:\n",
    "                stopped_frames_count += 1\n",
    "                #print(f\"MOTION BELOW THRESHOLD: {stopped_frames_count}/{stopped_frames_threshold}\")\n",
    "            else:\n",
    "                stopped_frames_count = 0\n",
    "                #print(f\"Motion detected: {distance:.2f} pixels\")\n",
    "        \n",
    "        # If not enough history, just check frame-to-frame\n",
    "        elif distance < motion_threshold:\n",
    "            stopped_frames_count += 1\n",
    "            #print(f\"MOTION BELOW THRESHOLD: {stopped_frames_count}/{stopped_frames_threshold}\")\n",
    "        else:\n",
    "            stopped_frames_count = 0\n",
    "            #print(f\"Motion detected: {distance:.2f} pixels\")\n",
    "    \n",
    "    # Update position history\n",
    "    position_history.append(current_position)\n",
    "    if len(position_history) > 100:\n",
    "        position_history.pop(0)\n",
    "    \n",
    "    # Check if we need to re-detect\n",
    "    need_to_redetect = stopped_frames_count >= stopped_frames_threshold\n",
    "    if need_to_redetect:\n",
    "        print(\"Motion stopped for too long - triggering re-detection\")\n",
    "    \n",
    "    # Return tracking status, updated history, and counter\n",
    "    return not need_to_redetect, position_history, stopped_frames_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d80530d-a6cc-4a86-9af7-7c1e263bb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pose():\n",
    "    \n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.90,\n",
    "                        model_complexity=2,)\n",
    "    \n",
    "    tracker = cv2.TrackerCSRT_create()\n",
    "    \n",
    "    video_path = '../input/video/right_profile_1.mov'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    rotated_width = height\n",
    "    rotated_height = width\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    # Get original video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Create VideoWriter object\n",
    "    output_path = '../output/video/pose_analysis_video1.avi'\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30, (rotated_width, rotated_height))\n",
    "    \n",
    "    frame_ratio = frame_rate/30\n",
    "    frame_idx = 0\n",
    "    bbox = ()\n",
    "    position_history = []\n",
    "    stopped_frames_count = 0\n",
    "\n",
    "    pass_count = 0 #number of frames where detection failed and logic passed\n",
    "    \n",
    "    try:  \n",
    "        while cap.isOpened():\n",
    "            # Read a frame from the video\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Can't read video frame. Exiting...\")\n",
    "                break\n",
    "                \n",
    "            if frame_idx == 0:\n",
    "                bbox = detect_ball(frame)\n",
    "                tracker.init(frame, bbox)  # Initialize only once\n",
    "            else:\n",
    "                success, bbox = tracker.update(frame)  # Update the tracker\n",
    "            \n",
    "                if success:\n",
    "                    p1 = (int(bbox[0]), int(bbox[1]))\n",
    "                    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "                    cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "                    \n",
    "                    center_x = int(bbox[0] + bbox[2]/2)\n",
    "                    center_y = int(bbox[1] + bbox[3]/2)\n",
    "                    current_position = (center_x, center_y)\n",
    "                    success, updated_position_history, stopped_frames_count = failed_track_check(position_history, current_position, stopped_frames_count)\n",
    "                    position_history = updated_position_history\n",
    "                    \n",
    "                    \n",
    "                if not success:\n",
    "                    # Tracking failure, reset tracking\n",
    "                    print(f\"resetting tracking (pass_count: {pass_count})\")\n",
    "                    try:\n",
    "                        # can change sensitivity if needed based on pass count\n",
    "                        param2_value = 40 if pass_count > 0 else 40\n",
    "                        #print(f\"Attempting detection with param2={param2_value}, pass_count={pass_count}\")\n",
    "                        \n",
    "                        new_bbox = detect_ball(frame, param2=param2_value)\n",
    "                        \n",
    "                        if new_bbox is not None:\n",
    "                            # Ball found\n",
    "                            bbox = new_bbox\n",
    "                            tracker = cv2.TrackerCSRT_create()\n",
    "                            success = tracker.init(frame, bbox)\n",
    "                            position_history = []\n",
    "                            stopped_frames_count = 0\n",
    "                            pass_count = 0  # Reset pass count on successful detection\n",
    "                            #print(f\"New ball detected with param2={param2_value} and tracking initialized\")\n",
    "                        else:\n",
    "                            # Detection failed, increment pass count\n",
    "                            pass_count += 1\n",
    "                            print(f\"Detection failed, pass_count now: {pass_count}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during ball detection: {e}\")\n",
    "                        pass_count += 1\n",
    "                        print(f\"Detection exception, pass_count now: {pass_count}\")\n",
    "\n",
    "            if frame_idx % frame_ratio < 1.0:\n",
    "\n",
    "                frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "                \n",
    "                # Convert BGR image to RGB\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "                # Process the image to find pose landmarks\n",
    "                results = pose.process(frame_rgb)\n",
    "                \n",
    "                # Draw pose landmarks on the frame\n",
    "                if results.pose_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame, \n",
    "                        results.pose_landmarks, \n",
    "                        mp_pose.POSE_CONNECTIONS\n",
    "                    )\n",
    "                out.write(frame)\n",
    "                \n",
    "                # Display the frame\n",
    "                cv2.imshow('MediaPipe Pose Detection', frame)\n",
    "                \n",
    "            frame_idx +=1\n",
    "            # Exit when 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    # Release resources\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Release resources (this will run even if there's an error)\n",
    "        \n",
    "        print(\"Cleaning up resources...\")\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        pose.close()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Force close any remaining windows\n",
    "        for i in range(5):  # Try multiple times\n",
    "            cv2.waitKey(1)\n",
    "        \n",
    "        print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a9704e-ddd6-4548-a199-e7f0d19cbc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744761192.484610 8881179 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Pro\n",
      "W0000 00:00:1744761192.531197 8882151 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744761192.556355 8882151 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744761192.648218 8882155 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-04-15 19:53:13.082 Python[86527:8881179] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-15 19:53:13.082 Python[86527:8881179] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't read video frame. Exiting...\n",
      "Cleaning up resources...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "draw_pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8d347-9823-4b32-a86d-c33eb379f005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (shot_analyzer)",
   "language": "python",
   "name": ".shot_analyzer_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
